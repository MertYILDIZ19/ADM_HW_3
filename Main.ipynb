{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM HW_3\n",
    "\n",
    "##### Mert YILDIZ / Nello Castaldo / Luca Avitabile\n",
    "\n",
    "\n",
    "\n",
    "## Q-1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following chunk you will find libraries needed for the execution of the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.error as uer\n",
    "from pathlib import Path\n",
    "from lxml import etree, html\n",
    "import regex as re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import io \n",
    "import codecs\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import stem\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import math\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in the following chunck we obtained the complete list of the 300 pages from which we took every 30 thousand book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 7825.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:24<00:00,  6.12s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 240878.91it/s]\n"
     ]
    }
   ],
   "source": [
    "url_rank = []\n",
    "for i in tqdm(range(1,301)):\n",
    "    url_rank.append('https://www.goodreads.com/list/show/1.Best_Books_Ever?page={}'.format(i))\n",
    "\n",
    "string_for_parsing = 'https://www.goodreads.com' #url of the site\n",
    "\n",
    "link_to_download = []\n",
    "for num_page in tqdm(url_rank):\n",
    "    book_prova  = requests.get(num_page)\n",
    "    page = soup(book_prova.text, 'html.parser')\n",
    "    domenica = page.find_all('a', {'class' : 'bookTitle'})\n",
    "    for item in domenica:\n",
    "        link_to_download.append(string_for_parsing+item['href'])\n",
    "\n",
    "#write txt file \n",
    "with open('urls.txt', 'w') as f:\n",
    "    for item in tqdm(link_to_download):\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "Path(\"directory\").mkdir(exist_ok=True)\n",
    "\n",
    "for i in range(1,4):\n",
    "    FolderName = 'folder{}'.format(i)\n",
    "    Path(FolderName).mkdir(exist_ok=True) #with this line we create folders for storing html files \n",
    "    \n",
    "def download_html():\n",
    "    id = int(input())  #we split download in 3 parts to work in parallel\n",
    "                        #Luca = 1  #Mert = 2  #Nello = 3\n",
    "    if id ==1:\n",
    "        a = 0\n",
    "        b = 10\n",
    "        i = 0\n",
    "    elif id ==2:\n",
    "        a = 11900\n",
    "        b = 20000\n",
    "        i = 99\n",
    "    elif id == 3:\n",
    "        a = 20000\n",
    "        b = 30000\n",
    "        i = 199\n",
    "    \n",
    "    for url in range(a,b):\n",
    "        \n",
    "        if url == 0 or url%100 == 0:\n",
    "            i += 1\n",
    "            folder_name = 'html_books_{}'.format(i)\n",
    "            Path(folder_name).mkdir(exist_ok=True)  #with this line we create folders for storing html files \n",
    "        urllib.request.urlretrieve(link_to_download[url], 'html_books_{}/article_{}.html'.format(i,url+1))\n",
    "        \n",
    "download_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this chunck is reported the code of the funtion which write tsv files from html files useful for search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_writer():\n",
    "    \n",
    "    print('Insert Path ')  #we grouped every pages' folders in a single folder to make every file more tidy in our pc\n",
    "    \n",
    "    path = input()\n",
    "    \n",
    "    print('Insert directory')  #first part of the name of each folder\n",
    "    \n",
    "    directory = input()\n",
    "    \n",
    "    count_error = []\n",
    "    \n",
    "    for j in range(0,301):  #in this for loop we take every book of every page\n",
    "        a = 1 + 100*(j)\n",
    "        b = 101 + 100*(j)\n",
    "        for k in range(a,b):\n",
    "            \n",
    "            try: #we applied this try/except because some html files are corrupted or missed\n",
    "\n",
    "                HtmlFile = open(path+'/'+directory+str(j)+'/article_{}.html'.format(k), 'r', encoding='utf-8')\n",
    "\n",
    "                source_code = HtmlFile.read()  #code to go into the html page\n",
    "\n",
    "                book = soup(source_code, 'html.parser')  #using beautiful soup\n",
    "                \n",
    "#now we take every single info needed using the path wrote in html document                \n",
    "\n",
    "                book_title = book.find('h1',{'id':'bookTitle'}).contents[0].strip()\n",
    "\n",
    "                book_series = book.find('h2',{'id':'bookSeries'})\n",
    "\n",
    "                if book_series.text.strip() == '':\n",
    "                    series = ''\n",
    "                else:\n",
    "                    series = book_series.contents[1].contents[0].strip()\n",
    "\n",
    "                author = book.find('span',{'itemprop':'name'}).contents[0]\n",
    "\n",
    "                rating = float(book.find('span',{'itemprop':'ratingValue'}).contents[0].strip())\n",
    "\n",
    "                number_of_rating = int(book.find('meta',{'itemprop':'ratingCount'}).contents[0].split()[0].replace(',',''))\n",
    "\n",
    "                review_count = int(book.find('meta',{'itemprop':'reviewCount'})['content'])\n",
    "\n",
    "                number_of_pages = int(book.find('span',{'itemprop':'numberOfPages'}).contents[0].split()[0])\n",
    "\n",
    "                publishing_date = book.find('div',{'class':'row'}).find_next_sibling().contents[0].split()[1:4]\n",
    "\n",
    "                plot = book.find('div',{'id':'description'})\n",
    "                \n",
    "                try:     #we needed this try/except because some book hasn't the plot            \n",
    "                \n",
    "                    complete_plot = plot.find_all('span')\n",
    "                    \n",
    "                    if detect(str(complete_plot)) != 'en':\n",
    "                        continue\n",
    "                \n",
    "                except: \n",
    "                    \n",
    "                    complete_plot = ''\n",
    "                    \n",
    "\n",
    "                details = book.find('div',{'id':'bookDataBox'})\n",
    "\n",
    "                places = details.find_all('a', {'href': re.compile(r'/places')})\n",
    "\n",
    "                settings = ''\n",
    "                for item in places:\n",
    "                    settings += item.text+','\n",
    "\n",
    "\n",
    "                character_sw = details.find_all('a', {'href': re.compile(r'/characters')})\n",
    "\n",
    "                character = ''\n",
    "                for item in character_sw:\n",
    "                    character+= item.text+','\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                count_error.append((a,j))\n",
    "                \n",
    "                continue\n",
    "\n",
    "\n",
    "            with open('books_tsv/article_{}.tsv'.format(k), 'wt', encoding=\"utf-8\") as out_file:\n",
    "                    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                    tsv_writer.writerow(['book_title','book_series','author','rating','number_of_rating',\n",
    "                                         'review_count','number_of_pages','publishing_date','complete_plot','places','characters'])\n",
    "                    tsv_writer.writerow([book_title,series,author,rating,number_of_rating,review_count,number_of_pages,\n",
    "                                        publishing_date,complete_plot,settings,character])                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert Path \n",
      "books\n",
      "Insert directory\n",
      "html_books_\n"
     ]
    }
   ],
   "source": [
    "tsv_writer()  #comand to lunch the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the books that match the query.\n",
    "\n",
    "First, we have pre-processed all the information collected for each book by\n",
    "\n",
    "* Removing stopwords\n",
    "* Removing punctuation\n",
    "* Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "\n",
    "    words = re.split(r'\\W+', str(text))   #split the text\n",
    "    words = [word.lower() for word in words]  #convert words to lower \n",
    "    without_punct = [wp for wp in words if wp not in punctuation] #remove punctuation\n",
    "    sw = stopwords.words('english')  #define stopwords\n",
    "    without_sw = [w for w in without_punct if w not in sw]  #remove stopwords \n",
    "    list_to_remove = ['b','br','span', 'one' , 'id', 'none' ] # define other unwanted words \n",
    "    clean_more = [w for w in without_sw if w not in list_to_remove] # remove them\n",
    "    clean_more_2 = [w for w in clean_more if  not (re.findall(re.compile(r'freetext'),w)\n",
    "                                                   or re.findall(re.compile(r'\\d'),w)) ]  #remove some tags that were not properly removed\n",
    "    \n",
    "    \n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed_list=[ps.stem(w) for w in clean_more_2 ]  # stemming\n",
    "    return ' '.join(stemmed_list) # merge all the words in the list and return a string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "\n",
    "#### 2.1.1) Create your index!\n",
    "\n",
    "Before building the index,\n",
    "\n",
    "We have created a file named vocabulary, in the .csv format, that maps each word to an integer (term_id).\n",
    "Then, the first brick of the homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "{\n",
    "\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "\n",
    "...}\n",
    "\n",
    "where document_i is the id of a document that contains the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=''  # string for all the books strings \n",
    "for books in range(1,30001): \n",
    "    text_1='' # string for each book string\n",
    "    try:\n",
    "        df = pd.read_csv(\"books_3_tsv/article_\"+str(books)+\".tsv\", sep='\\t', encoding='utf-8' ) # read tsv file of book\n",
    "    except:\n",
    "        continue\n",
    "    df = df.fillna(\"\")  #fill nan info \n",
    "    df['book_title']=df['book_title'].apply(cleaner)  # clean the title\n",
    "    df['complete_plot']=df['complete_plot'].apply(cleaner) #clean the plot \n",
    "    \n",
    "\n",
    "    #df.to_csv('cleaned_csv/prova_libro_'+str(books)+'.csv') # create new cleaned csv using this line, run only once\n",
    "    \n",
    "    #now create the vocabulary\n",
    "    \n",
    "    title = df.iloc[0, 0]  #get the title \n",
    "    plot = df.iloc[0, 8] #get the plot\n",
    "    text = text + \" \" + title + \" \" + plot # update final string \n",
    "    text_1 = title + \" \" + plot #update  local string \n",
    "    arr_1=set(text_1.split()) #create a set from the local splitted string \n",
    "    file = open(\"txt_files/article_\"+str(books)+\".txt\", \"w\") #create a txt file for each book that contains the set \n",
    "    file.write(str(arr_1))  #write the file\n",
    "    file.close()  #close the file \n",
    "\n",
    "arr = set(text.split()) # use set to eliminate repeating words from final string \n",
    "vocab = pd.DataFrame(arr) # create a dataframe from the set \n",
    "vocab.to_csv(\"vocab.csv\") #save vocabulary as csv with indices for each vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voc= pd.read_csv('vocab.csv')  # read the vocabulary just created \n",
    "\n",
    "df_voc.columns=['id','word'] # change columns names\n",
    "\n",
    "d = defaultdict(set) # create a new defaultdict to save the iverted index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words_2(id): # read the txt files of the documents \n",
    "    try:\n",
    "        \n",
    "        file = open(\"txt_files/article_\"+str(doc)+ \".txt\", \"r\") \n",
    "        test=ast.literal_eval(file.read()) #evaluate the set in the txt file \n",
    "        file.close()\n",
    "        return test #return the set evaluated \n",
    "    except:\n",
    "        return [] #return empty list if some errors occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(1,30001): #fill the inverted index \n",
    "    test=extract_words_2(doc) # get the book set of words \n",
    "    for item in test: \n",
    "       \n",
    "        d[item].add(doc)  #update the list of documents of word 'item'\n",
    "    \n",
    "\n",
    "dic_keys=d.keys()  #words in the inverted index \n",
    "new_dic=defaultdict(set) #new inverted index to translate word into id\n",
    "for item in dic_keys: \n",
    "    try:\n",
    "        new_key=df_voc[df_voc.word==item].id.iloc[0] # get the id of the word from vocabulary\n",
    "        new_dic[new_key]=d[item] # copy the list of documents in the new inverted index with id as key\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "file = open(\"inverted_index_1.txt\", \"w\")  #save inverted index as txt file \n",
    "file.write(str(new_dic))\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query():\n",
    "    query= str(input('Insert the query ')) #ask for the query\n",
    "    final_query=cleaner(query) # preprocess the query\n",
    "    word_list=final_query.split() #get the list of words of the query\n",
    "    key_list=[] # list to save id of the words in the query\n",
    "    for word in word_list:\n",
    "        key=df_voc[df_voc.word==word].id.iloc[0] #get id of each word  from vocabulary\n",
    "        key_list.append(key) #add to key_list\n",
    "    try:\n",
    "        result=new_dic[key_list[0]] #get the list of documents of first word of the query\n",
    "    except:\n",
    "        print('No results found')\n",
    "        return [] #if key_list is empty return an empty list\n",
    "    for item in key_list: # execute the intersection between all the lists of documents \n",
    "        result=result.intersection(new_dic[item]) #update result with intersection\n",
    "        \n",
    "    return result #return the documents that contains all the words of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert the query winnie\n"
     ]
    }
   ],
   "source": [
    "books=search_query()  #call the function and save the result in books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_link(index): #given the doc index return the link \n",
    "    ind=str(index) \n",
    "   \n",
    "    folder= math.floor((index-1) /100) # find the folder that contains the html file of the book\n",
    "    HtmlFile = open('books_3/html_books_'+str(folder)+'/article_{}.html'.format(ind), 'r', encoding='utf-8') #open the file \n",
    "    source_code = HtmlFile.read() #read it \n",
    "\n",
    "    book = soup(source_code, 'html.parser') # parse the content of the file \n",
    "\n",
    "    book_link = book.find('link') # find the link in link element \n",
    "    \n",
    "    return book_link['href'] #return the value of href attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walt Disney's Winnie-the-Pooh and the Honey Patch : nan https://www.goodreads.com/book/show/6394554-walt-disney-s-winnie-the-pooh-and-the-honey-patch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A Dragon's Guide to the Care and Feeding of Humans :  Crusty dragon Miss Drake has a new pet human precocious Winnie Oddly enough Winnie seems to think Miss Drake is her pet a ridiculous notion Unknown to most of its inhabitants the City by the Bay is home to many mysterious and fantastic creatures hidden beneath the parks among the clouds and even in plain sight And Winnie wants to draw every new creature she encoun style display Crusty dragon Miss Drake has a new pet human precocious Winnie Oddly enough Winnie seems to think Miss Drake is her pet a ridiculous notion Unknown to most of its inhabitants the City by the Bay is home to many mysterious and fantastic creatures hidden beneath the parks among the clouds and even in plain sight And Winnie wants to draw every new creature she encounters the good the bad and the ugly But Winnie s sketchbook is not what it seems Somehow her sketchlings have been set loose on the city streets It will take Winnie and Miss Drake s combined efforts to put an end to the mayhem before it s too late  https://www.goodreads.com/book/show/22551747-a-dragon-s-guide-to-the-care-and-feeding-of-humans\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in books:  # for all the books in the result of the function \n",
    "    df = pd.read_csv(\"books_3_tsv/article_\"+str(index)+\".tsv\", sep='\\t', encoding='utf-8' ) #read the tsv file \n",
    "    plot_split=re.split(r'\\W+', str(df['complete_plot'].iloc[0])) # split the plot \n",
    "    title=df['book_title'].iloc[0] # get the title \n",
    "    list_to_remove = ['b','br','span', 'one' , 'id', 'none','[',']'] # words that will be removed \n",
    "    clean_more = [w for w in plot_split if w not in list_to_remove] # remove them   \n",
    "    clean_more_2 = [w for w in clean_more if  not (re.findall(re.compile(r'freetext'),w)\n",
    "                                                   or re.findall(re.compile(r'\\d'),w)) ] # remove other unwanted elements \n",
    "    url=find_link(index) #find the link of the book\n",
    "    \n",
    "    print(title, ':' ,' '.join(clean_more_2),url) # print the title, the plot and the url \n",
    "    print('\\n\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "for books in range(1,30001): # create a counter for each book\n",
    "    text_1='' # string of words in the document\n",
    "    try:\n",
    "        df = pd.read_csv(\"books_3_tsv/article_\"+str(books)+\".tsv\", sep='\\t', encoding='utf-8' ) #read tsv files\n",
    "    except:\n",
    "        continue\n",
    "    df = df.fillna(\"\") #fill empty values \n",
    "    df['book_title']=df['book_title'].apply(cleaner) # clean the title \n",
    "    df['complete_plot']=df['complete_plot'].apply(cleaner) #clean the book\n",
    "     \n",
    "\n",
    "    #df.to_csv('cleaned_csv/prova_libro_'+str(books)+'.csv') #to create new cleaned csv, run once \n",
    "    \n",
    "    \n",
    "    \n",
    "    title = df.iloc[0, 0] # get the title \n",
    "    plot = df.iloc[0, 8] #get the plot \n",
    "    text_1 = title + \" \" + plot # update the text of the document \n",
    "    arr_1=(text_1.split()) # split the text string \n",
    "    c = Counter(arr_1)  # create a dictionary with the number of occurences for each word\n",
    "    file = open(\"txt_files_2/article_\"+str(books)+\".txt\", \"w\")  # save it in a txt files \n",
    "    file.write(str(c)) \n",
    "    file.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(list)  #create a defaultdict. keys: word , values: (document id, occurrences of the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_counter(id): #given the index of the book it reads the dictionary in txt file and returns it \n",
    "    try:\n",
    "        \n",
    "        file = open(\"txt_files_2/article_\"+str(doc)+ \".txt\", \"r\") #open the file \n",
    "        dic=file.read() # read it \n",
    "        prova=dic.replace('Counter(','').replace(')','') # clean the content to be evaluated \n",
    "        test=ast.literal_eval(prova) #evaluate the dictionary saved as a string in the txt file \n",
    "        file.close()\n",
    "        return test\n",
    "    except:\n",
    "        return {} # return empty dictionary if some errors occur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(1,30001):   #fill the defaultdict d with all the documents\n",
    "    result=extract_from_counter(doc) #get the Counter \n",
    "    for key in result.keys(): # update dictionary \n",
    "        heapq.heappush(d[key],(result[key],doc)) # using heap to store values and to keep the order\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_keys=d.keys() # create a new dictionary to subsitute words with words_id\n",
    "new_dic=defaultdict(list) \n",
    "for item in dic_keys:\n",
    "    try:\n",
    "        new_key=df_voc[df_voc.word==item].id.iloc[0] # find the word id from the vocabulary\n",
    "        new_dic[new_key]=d[item] # copy the values in the new dictionary\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the lenght of each document in a dictionary\n",
    "dic_lenght=defaultdict(int)\n",
    "\n",
    "for doc in range(1,30001): # fill dic_lenght with all the documents lenghts \n",
    "    result=extract_from_counter(doc) #get the Counter \n",
    "    lenght=sum(result.values()) # sum the values of the Counter of the document \n",
    "    if(lenght==0):\n",
    "        continue\n",
    "    dic_lenght[doc]=lenght #update the value in the dictionary\n",
    "documents_number=30000 # numbers of documents in the collection\n",
    "\n",
    "# save the number of documents for each word in a different dictionary\n",
    "documents_with_word= defaultdict(int) \n",
    "for item in d.keys(): \n",
    "    documents_with_word[item]=len(d[item]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create again the dictionary with the heap ordered by tf_idf\n",
    "final_inverted_index=defaultdict(list) #this is the final inverted indev with words as keys.\n",
    "for doc in range(1,30001):\n",
    "    result=extract_from_counter(doc)\n",
    "    doc_len=dic_lenght[doc] # get the len of the document \n",
    "    for key in result.keys(): \n",
    "        tf= result[key] / doc_len\n",
    "        idf= math.log(documents_number / documents_with_word[key] ,10) \n",
    "        tf_idf= tf * idf #find tfidf\n",
    "        heapq.heappush(final_inverted_index[key],(tf_idf,doc)) # the list is ordered by the tfidf using a heap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the word with the numbers  \n",
    "dic_keys=final_inverted_index.keys()\n",
    "inverted_index_tfidf=defaultdict(list)  #final inverted index\n",
    "for item in dic_keys:\n",
    "    try:\n",
    "        new_key=df_voc[df_voc.word==item].id.iloc[0] #get the id of the word from vocabulary\n",
    "        inverted_index_tfidf[new_key]=final_inverted_index[item] # copy the value of the ol dictionary in the final inverted index\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open('inverted_index_1.txt','r') #read the inverted index of the previous search engine \n",
    "obj= file.read()\n",
    "new_dic=ast.literal_eval(obj) #evaluate string as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tfidf(word_list): # given a list, this functions returns the tfidf_vector for the list of words \n",
    "    c=Counter(word_list)  # number of occurences of the word \n",
    "    res_vec=[] \n",
    "    string_len=len(word_list) # len of the list \n",
    "    for word in word_list: # calculate the tfidf for all the word in the query \n",
    "        tf=c[word]/string_len\n",
    "        idf= math.log(documents_number / documents_with_word[word] ,10) \n",
    "        res_vec.append(tf*idf) # append the result to the vector \n",
    "    return res_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process the query\n",
    "def search_query(): #the search engine is similar to the previous one, but in this case we use it to filter the list of\n",
    "                    # documents that contains all the words of the query. We will apply the cosine similarity function to this books\n",
    "    query= str(input('Insert the query '))\n",
    "    final_query=cleaner(query) #clean the query\n",
    "    word_list=final_query.split() #list of words \n",
    "    tf_idf=find_tfidf(word_list) # tfidf vector of the query\n",
    "    key_list=[]\n",
    "    for word in word_list: # find the id of each word in the query using vocabulary\n",
    "        key=df_voc[df_voc.word==word].id.iloc[0]\n",
    "        key_list.append(key) #update list of id \n",
    "    try:\n",
    "        result=new_dic[key_list[0]]  # get the list of documentsof first word \n",
    "    except:\n",
    "        print('No results found for the input query') # if it is empty return \n",
    "        return 0,0,0\n",
    "   \n",
    "    for item in key_list:\n",
    "        result=result.intersection(new_dic[item]) # get the intersection between all the list of documents \n",
    "        \n",
    "    return result,key_list,tf_idf  #it returns the set of documents, the words in the query, and the tf_idf vector of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert the query love hard pain\n"
     ]
    }
   ],
   "source": [
    "res,key_list,tf_vector=search_query()   # call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list=[] \n",
    "for book in res: # for each book in result we create the tfidf_vector. The lenght of the vector is equal to the length \n",
    "                # of the tfidf_vector of the query\n",
    "    vec=[]\n",
    "    for key in key_list:\n",
    "        data_frame= pd.DataFrame(inverted_index_tfidf[key])#convert the values of the key in a dataframe \n",
    "        data_frame.columns=['tf','doc']\n",
    "        vec.append(data_frame[data_frame.doc== book].tf.iloc[0]) # search the tfidf value filtering the dataframe \n",
    "   \n",
    "    similarity= 1 - spatial.distance.cosine(tf_vector,vec) # for each book we apply the cosine similarity\n",
    "    result_list.append((similarity, book)) # store the result in a list\n",
    "\n",
    "final_list=heapq.nlargest(4,result_list)  # get the top 4 results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe= pd.DataFrame(columns=['Title','Plot','Url','Similarity']) #create a dataframe to store the results \n",
    "#create a list for each field  \n",
    "title_list=[]\n",
    "plot_list=[]\n",
    "url_list=[]\n",
    "sim_list=[]\n",
    "for similar,index in final_list:  # for each book we print the title, the plot, the url and the cosine similarity\n",
    "    df = pd.read_csv(\"books_3_tsv/article_\"+str(index)+\".tsv\", sep='\\t', encoding='utf-8' ) #read the file \n",
    "    plot_split=re.split(r'\\W+', str(df['complete_plot'].iloc[0])) # split the plot \n",
    "    title=df['book_title'].iloc[0] #get the title \n",
    "    list_to_remove = ['b','br','span', 'one' , 'id', 'none','[',']'] \n",
    "    clean_more = [w for w in plot_split if w not in list_to_remove] \n",
    "    clean_more_2 = [w for w in clean_more if  not (re.findall(re.compile(r'freetext'),w)\n",
    "                                                   or re.findall(re.compile(r'\\d'),w)) ] # clean again the plot\n",
    "    url=find_link(index) #find the link \n",
    "    #update the different lists \n",
    "    title_list.append(title) \n",
    "    plot_list.append(' '.join(clean_more_2))\n",
    "    url_list.append(url)\n",
    "    sim_list.append(similar)\n",
    "#update the dataframe using the lists \n",
    "result_dataframe['Title'] = title_list\n",
    "result_dataframe['Plot'] = plot_list\n",
    "result_dataframe['Url'] = url_list\n",
    "result_dataframe['Similarity']=sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unbearable Lightness: A Story of Loss and Gain</td>\n",
       "      <td>I didn t decide to become anorexic It snuck u...</td>\n",
       "      <td>https://www.goodreads.com/book/show/9219901-un...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Most of All You</td>\n",
       "      <td>i A broken woman i Crystal learned long ago t...</td>\n",
       "      <td>https://www.goodreads.com/book/show/32854499-m...</td>\n",
       "      <td>0.969612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>More Than Two: A Practical Guide to Ethical Po...</td>\n",
       "      <td>Can you love more than person Have multiple r...</td>\n",
       "      <td>https://www.goodreads.com/book/show/21955937-m...</td>\n",
       "      <td>0.956397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Unexpected Blessing</td>\n",
       "      <td>i This is an alternate cover edition of ISBN ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/52374459-a...</td>\n",
       "      <td>0.955570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0     Unbearable Lightness: A Story of Loss and Gain   \n",
       "1                                    Most of All You   \n",
       "2  More Than Two: A Practical Guide to Ethical Po...   \n",
       "3                             An Unexpected Blessing   \n",
       "\n",
       "                                                Plot  \\\n",
       "0   I didn t decide to become anorexic It snuck u...   \n",
       "1   i A broken woman i Crystal learned long ago t...   \n",
       "2   Can you love more than person Have multiple r...   \n",
       "3   i This is an alternate cover edition of ISBN ...   \n",
       "\n",
       "                                                 Url  Similarity  \n",
       "0  https://www.goodreads.com/book/show/9219901-un...    1.000000  \n",
       "1  https://www.goodreads.com/book/show/32854499-m...    0.969612  \n",
       "2  https://www.goodreads.com/book/show/21955937-m...    0.956397  \n",
       "3  https://www.goodreads.com/book/show/52374459-a...    0.955570  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Q-2 Search Engine with tfidf-cos similarity (Built-in functions)\n",
    "##### For this question we have another solution which uses the built-in functions. Please see the solution and code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv files for all books with book title, cleaned plot, url and empty similarity column.\n",
    "# The similarity score will be added to dataframe later\n",
    "\n",
    "df_new = pd.DataFrame(columns = ['Book Title', 'Plot', 'Url','Similarity'])\n",
    "title_list = []\n",
    "plot_list = []\n",
    "url_list = []\n",
    "\n",
    "for index in tqdm(range(1,30000)):\n",
    "    try:\n",
    "        df = pd.read_csv(\"books_tsv/article_\"+str(index)+\".tsv\", sep='\\t', encoding='utf-8' )\n",
    "    except:\n",
    "        continue\n",
    "    df['book_title'] = df['book_title'].apply(cleaner)\n",
    "    title = df['book_title'].iloc[0]\n",
    "    df['complete_plot']=df['complete_plot'].apply(cleaner)\n",
    "    plot = df['complete_plot'].iloc[0]\n",
    "    complete_plt= title + \" \" + plot\n",
    "    plot_split = re.split(r'\\W+', str(complete_plt))\n",
    "    \n",
    "    url=find_link(index)    \n",
    "    title_list.append(title)\n",
    "    plot_list.append(' '.join(plot_split))\n",
    "    url_list.append(url)\n",
    "    \n",
    "df_new['Book Title'] = title_list\n",
    "df_new['Plot'] = plot_list\n",
    "df_new['Url'] = url_list\n",
    "df_new.to_csv ('tfidf_cos.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search something: last freedom\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18331</th>\n",
       "      <td>first last freedom</td>\n",
       "      <td>first last freedom krishnamurti lead spiritu t...</td>\n",
       "      <td>https://www.goodreads.com/book/show/64710.The_...</td>\n",
       "      <td>0.427591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8203</th>\n",
       "      <td>love freedom alon koan relationship</td>\n",
       "      <td>love freedom alon koan relationship today worl...</td>\n",
       "      <td>https://www.goodreads.com/book/show/97008.Love...</td>\n",
       "      <td>0.426060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11982</th>\n",
       "      <td>found freedom unfre world handbook person liberti</td>\n",
       "      <td>found freedom unfre world handbook person libe...</td>\n",
       "      <td>https://www.goodreads.com/book/show/82104.How_...</td>\n",
       "      <td>0.355028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22317</th>\n",
       "      <td>seed america trilog chain forg ash</td>\n",
       "      <td>seed america trilog chain forg ash would risk ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/29430512-t...</td>\n",
       "      <td>0.318160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4156</th>\n",
       "      <td>diari freedom writer experi</td>\n",
       "      <td>diari freedom writer experi survivor word cont...</td>\n",
       "      <td>https://www.goodreads.com/book/show/17879512-d...</td>\n",
       "      <td>0.291967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361</th>\n",
       "      <td>escap freedom</td>\n",
       "      <td>escap freedom human cannot live danger respons...</td>\n",
       "      <td>https://www.goodreads.com/book/show/25491.Esca...</td>\n",
       "      <td>0.279686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19841</th>\n",
       "      <td>develop freedom</td>\n",
       "      <td>develop freedom winner nobel prize econom esse...</td>\n",
       "      <td>https://www.goodreads.com/book/show/173961.Dev...</td>\n",
       "      <td>0.265597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15236</th>\n",
       "      <td>capit freedom</td>\n",
       "      <td>capit freedom select time literari supplement ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/51877.Capi...</td>\n",
       "      <td>0.255351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19386</th>\n",
       "      <td>ann mccaffrey freedom collect freedom land fre...</td>\n",
       "      <td>ann mccaffrey freedom collect freedom land fre...</td>\n",
       "      <td>https://www.goodreads.com/book/show/24867.Anne...</td>\n",
       "      <td>0.240459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17584</th>\n",
       "      <td>escap</td>\n",
       "      <td>escap nearli ten year captiv grace readi make ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/21470020-e...</td>\n",
       "      <td>0.229970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Book Title  \\\n",
       "18331                                 first last freedom   \n",
       "8203                 love freedom alon koan relationship   \n",
       "11982  found freedom unfre world handbook person liberti   \n",
       "22317                 seed america trilog chain forg ash   \n",
       "4156                         diari freedom writer experi   \n",
       "4361                                       escap freedom   \n",
       "19841                                    develop freedom   \n",
       "15236                                      capit freedom   \n",
       "19386  ann mccaffrey freedom collect freedom land fre...   \n",
       "17584                                              escap   \n",
       "\n",
       "                                                    Plot  \\\n",
       "18331  first last freedom krishnamurti lead spiritu t...   \n",
       "8203   love freedom alon koan relationship today worl...   \n",
       "11982  found freedom unfre world handbook person libe...   \n",
       "22317  seed america trilog chain forg ash would risk ...   \n",
       "4156   diari freedom writer experi survivor word cont...   \n",
       "4361   escap freedom human cannot live danger respons...   \n",
       "19841  develop freedom winner nobel prize econom esse...   \n",
       "15236  capit freedom select time literari supplement ...   \n",
       "19386  ann mccaffrey freedom collect freedom land fre...   \n",
       "17584  escap nearli ten year captiv grace readi make ...   \n",
       "\n",
       "                                                     Url  Similarity  \n",
       "18331  https://www.goodreads.com/book/show/64710.The_...    0.427591  \n",
       "8203   https://www.goodreads.com/book/show/97008.Love...    0.426060  \n",
       "11982  https://www.goodreads.com/book/show/82104.How_...    0.355028  \n",
       "22317  https://www.goodreads.com/book/show/29430512-t...    0.318160  \n",
       "4156   https://www.goodreads.com/book/show/17879512-d...    0.291967  \n",
       "4361   https://www.goodreads.com/book/show/25491.Esca...    0.279686  \n",
       "19841  https://www.goodreads.com/book/show/173961.Dev...    0.265597  \n",
       "15236  https://www.goodreads.com/book/show/51877.Capi...    0.255351  \n",
       "19386  https://www.goodreads.com/book/show/24867.Anne...    0.240459  \n",
       "17584  https://www.goodreads.com/book/show/21470020-e...    0.229970  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file created above\n",
    "\n",
    "df_new = pd.read_csv(\"tfidf_cos.csv\")\n",
    "\n",
    "# Calculate the tfidf and cosine similarity score for all books with the query\n",
    "\n",
    "vectorizer = TfidfVectorizer() # Get tf-idf matrix using fit_transform function\n",
    "X = vectorizer.fit_transform(df_new['Plot'].values.astype('U')) # Store tf-idf representations of all docs\n",
    "\n",
    "query = str(input('Search something: ')) # Ask the user to write the query\n",
    "\n",
    "query_vec = vectorizer.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
    "results = cosine_similarity(X,query_vec).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
    "cos_sim = results.tolist() # Convert the cosine similarity result to list format\n",
    "df_new['Similarity'] = cos_sim # Write the similarity score to Similarity column of dataframe\n",
    "    \n",
    "df_new.nlargest(10,'Similarity') # Take the first 10 most similar books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-3. Define a new score!\n",
    "\n",
    "As it has been requested we have added a new filter. We are asking to user to enter the query and the minimum rating for the books. We have defined a new score which is taking the rating into account and normalize it then multiply by cosine similarity. Then we are showing the 10 books with the highest new score that has been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 29999/29999 [56:40<00:00,  8.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a csv files for all books with cleaned book title, rating, cleaned plot, url, empty similarity and score column.\n",
    "# The similarity and new score that has been defined will be added to dataframe later.\n",
    "\n",
    "df_new = pd.DataFrame(columns = ['Book Title','Rating', 'Number_of_Rating', 'Plot', 'Url', 'Similarity', 'Score'])\n",
    "title_list = []\n",
    "plot_list = []\n",
    "url_list = []\n",
    "rating_list = []\n",
    "num_rating_list = []\n",
    "\n",
    "for index in tqdm(range(1,30000)):\n",
    "    try:\n",
    "        df = pd.read_csv(\"books_tsv/article_\"+str(index)+\".tsv\", sep='\\t', encoding='utf-8' )\n",
    "    except:\n",
    "        continue\n",
    "    df['book_title'] = df['book_title'].apply(cleaner)\n",
    "    title = df['book_title'].iloc[0]\n",
    "    df['complete_plot']=df['complete_plot'].apply(cleaner)\n",
    "    plot = df['complete_plot'].iloc[0]\n",
    "    complete_plt= title + \" \" + plot\n",
    "    plot_split = re.split(r'\\W+', str(complete_plt))\n",
    "    rate = df['rating'].iloc[0]/5 # Normalize the rating\n",
    "    num_rating = df['number_of_rating'].iloc[0]\n",
    "    \n",
    "    url=find_link(index)    \n",
    "    title_list.append(title)\n",
    "    plot_list.append(' '.join(plot_split))\n",
    "    url_list.append(url)\n",
    "    rating_list.append(rate)\n",
    "    num_rating_list.append(num_rating)\n",
    "    \n",
    "df_new['Book Title'] = title_list\n",
    "df_new['Plot'] = plot_list\n",
    "df_new['Url'] = url_list\n",
    "df_new['Rating'] = rating_list\n",
    "df_new['Number_of_Rating'] = num_rating_list\n",
    "df_new.to_csv ('tfidf_cos2.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search something: love life\n",
      "Minimum rating: 4\n",
      "Minimum number of rating: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Number_of_Rating</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>thief</td>\n",
       "      <td>0.874</td>\n",
       "      <td>34377</td>\n",
       "      <td>thief note self love patient love kind love bo...</td>\n",
       "      <td>https://www.goodreads.com/book/show/16090981-t...</td>\n",
       "      <td>0.380172</td>\n",
       "      <td>0.332270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>art love</td>\n",
       "      <td>0.806</td>\n",
       "      <td>53172</td>\n",
       "      <td>art love fiftieth anniversari edit groundbreak...</td>\n",
       "      <td>https://www.goodreads.com/book/show/14142.The_...</td>\n",
       "      <td>0.381103</td>\n",
       "      <td>0.307169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>love languag secret love last</td>\n",
       "      <td>0.850</td>\n",
       "      <td>300738</td>\n",
       "      <td>love languag secret love last div p simpl idea...</td>\n",
       "      <td>https://www.goodreads.com/book/show/23878688-t...</td>\n",
       "      <td>0.343241</td>\n",
       "      <td>0.291755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>four love</td>\n",
       "      <td>0.828</td>\n",
       "      <td>44040</td>\n",
       "      <td>four love four love summar four kind human lov...</td>\n",
       "      <td>https://www.goodreads.com/book/show/29938407-t...</td>\n",
       "      <td>0.323872</td>\n",
       "      <td>0.268166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7267</th>\n",
       "      <td>lilli</td>\n",
       "      <td>0.854</td>\n",
       "      <td>27827</td>\n",
       "      <td>lilli cash mayson forc choos love life unborn ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/18781540-u...</td>\n",
       "      <td>0.255823</td>\n",
       "      <td>0.218473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>cours love</td>\n",
       "      <td>0.810</td>\n",
       "      <td>25785</td>\n",
       "      <td>cours love engross tale provid plenti food tho...</td>\n",
       "      <td>https://www.goodreads.com/book/show/27845690-t...</td>\n",
       "      <td>0.251856</td>\n",
       "      <td>0.204003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.852</td>\n",
       "      <td>1158061</td>\n",
       "      <td>new york time bestsel author giver star disco...</td>\n",
       "      <td>https://www.goodreads.com/book/show/17347634-m...</td>\n",
       "      <td>0.221267</td>\n",
       "      <td>0.188520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>crash</td>\n",
       "      <td>0.910</td>\n",
       "      <td>41104</td>\n",
       "      <td>crash life crash around us hard will fight thi...</td>\n",
       "      <td>https://www.goodreads.com/book/show/30166123-c...</td>\n",
       "      <td>0.202856</td>\n",
       "      <td>0.184599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8946</th>\n",
       "      <td>love dare</td>\n",
       "      <td>0.878</td>\n",
       "      <td>52135</td>\n",
       "      <td>love dare new york time best seller million un...</td>\n",
       "      <td>https://www.goodreads.com/book/show/4499669-th...</td>\n",
       "      <td>0.191880</td>\n",
       "      <td>0.168471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19191</th>\n",
       "      <td>citi girl</td>\n",
       "      <td>0.808</td>\n",
       "      <td>160745</td>\n",
       "      <td>citi girl new york time bestsel author eat pra...</td>\n",
       "      <td>https://www.goodreads.com/book/show/51918871-c...</td>\n",
       "      <td>0.207864</td>\n",
       "      <td>0.167954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Book Title  Rating  Number_of_Rating  \\\n",
       "2216                           thief   0.874             34377   \n",
       "2684                        art love   0.806             53172   \n",
       "877    love languag secret love last   0.850            300738   \n",
       "6749                       four love   0.828             44040   \n",
       "7267                           lilli   0.854             27827   \n",
       "10660                     cours love   0.810             25785   \n",
       "339                              NaN   0.852           1158061   \n",
       "4083                           crash   0.910             41104   \n",
       "8946                       love dare   0.878             52135   \n",
       "19191                      citi girl   0.808            160745   \n",
       "\n",
       "                                                    Plot  \\\n",
       "2216   thief note self love patient love kind love bo...   \n",
       "2684   art love fiftieth anniversari edit groundbreak...   \n",
       "877    love languag secret love last div p simpl idea...   \n",
       "6749   four love four love summar four kind human lov...   \n",
       "7267   lilli cash mayson forc choos love life unborn ...   \n",
       "10660  cours love engross tale provid plenti food tho...   \n",
       "339     new york time bestsel author giver star disco...   \n",
       "4083   crash life crash around us hard will fight thi...   \n",
       "8946   love dare new york time best seller million un...   \n",
       "19191  citi girl new york time bestsel author eat pra...   \n",
       "\n",
       "                                                     Url  Similarity     Score  \n",
       "2216   https://www.goodreads.com/book/show/16090981-t...    0.380172  0.332270  \n",
       "2684   https://www.goodreads.com/book/show/14142.The_...    0.381103  0.307169  \n",
       "877    https://www.goodreads.com/book/show/23878688-t...    0.343241  0.291755  \n",
       "6749   https://www.goodreads.com/book/show/29938407-t...    0.323872  0.268166  \n",
       "7267   https://www.goodreads.com/book/show/18781540-u...    0.255823  0.218473  \n",
       "10660  https://www.goodreads.com/book/show/27845690-t...    0.251856  0.204003  \n",
       "339    https://www.goodreads.com/book/show/17347634-m...    0.221267  0.188520  \n",
       "4083   https://www.goodreads.com/book/show/30166123-c...    0.202856  0.184599  \n",
       "8946   https://www.goodreads.com/book/show/4499669-th...    0.191880  0.168471  \n",
       "19191  https://www.goodreads.com/book/show/51918871-c...    0.207864  0.167954  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the tfidf and cosine similarity score for all books with the query\n",
    "df_new = pd.read_csv(\"tfidf_cos2.csv\")\n",
    "\n",
    "vectorizer = TfidfVectorizer() # Get tf-idf matrix using fit_transform function\n",
    "X = vectorizer.fit_transform(df_new['Plot'].values.astype('U')) # Store tf-idf representations of all docs\n",
    "\n",
    "query = str(input('Search something: ')) # Ask the user to write the query\n",
    "min_rating = np.float(input('Minimum rating: ')) # We have asked to user to write the minimum rating he/she wants\n",
    "min_number_rating = np.int(input('Minimum number of rating: ')) # We have asked to user to write the minimum rating he/she wants\n",
    "\n",
    "df_empty = pd.DataFrame(columns = ['Book Title', 'Rating', 'Number_of_Rating', 'Plot', 'Url', 'Similarity', 'Score'])\n",
    "\n",
    "query_vec = vectorizer.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
    "results = cosine_similarity(X,query_vec).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
    "cos_sim = results.tolist() # Convert the cosine similarity result to list format\n",
    "df_new['Similarity'] = cos_sim\n",
    "df_new['Score'] = cos_sim * df_new['Rating']  # Calculate the score as cosine_similarity*rating and write to Score column of dataframe\n",
    "\n",
    "df_new[(df_new['Number_of_Rating'] > min_number_rating) & (df_new['Rating']*5 > min_rating)].nlargest(10,'Score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-5. Algorithmic Question\n",
    "\n",
    "You are given a string written in english capital letters, for example S=\"CADFECEILGJHABNOPSTIRYOEABILCNR.\" You are asked to find the maximum length of a subsequence of characters that is in alfabetical order. For example, here a subsequence of characters in alphabetical order is the \"ACEGJSTY\": \"CADFECEILGJHABNOFPSTIRYOEABILCNR.\" Among all the possible such sequences, you are asked to find the one that is the longest.\n",
    "\n",
    "1. Write a recursive program that, given a string, computes the length of the subsequence of maximum length that is in alphabetical order. Try some examples. Are the examples of short strings correct? Can you find examples that your algorithm does not terminate in reasonable time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n",
      "Please enter a sequence with upper cases: ABUOMLDEF\n",
      "Length of the alphabetical ordered longest subsequence is  5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "def recursive_lcs(Y, X, m, n): \n",
    "  \n",
    "    if m == 0 or n == 0: \n",
    "        return 0; \n",
    "    elif Y[m-1] == X[n-1]: \n",
    "        return 1 + recursive_lcs(Y, X, m-1, n-1); \n",
    "    else: \n",
    "        return max(recursive_lcs(Y, X, m, n-1), recursive_lcs(Y, X, m-1, n)); \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    " \n",
    "Alphabet_S = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "Input_S =  input(\"Please enter a sequence with upper cases: \")\n",
    "#print (\"Length of LCS is \", lcs(Y , X, len(Y), len(X)))\n",
    "\n",
    "if Input_S.isalpha() and Input_S.isupper():\n",
    "    print (\"Length of the alphabetical ordered longest subsequence is \", recursive_lcs(Alphabet_S, Input_S, len(Alphabet_S), len(Input_S)))\n",
    "else:\n",
    "    print ('Please enter a string with only upper case letters in English!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideration:\n",
    "\n",
    "As we have tried several times with short strings that can prove whether the function is finding correct answer or not, our function has returned correct answer for all different inputs. Since we have conditioned the input that all strings can only contain the letters of English alphabet it also makes sure that we have a correct input. We have tried the function by increasing the length of string and when we have tried a string with length 20 it just kept running and did not returned a result in a reasonable time.\n",
    "\n",
    "2. Show that the running time of the algorithm is exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Execution Time & Length of Query Plot')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def recursive_lcs(Y, X, m, n): \n",
    "  \n",
    "    if m == 0 or n == 0: \n",
    "        return 0; \n",
    "    elif Y[m-1] == X[n-1]: \n",
    "        return 1 + recursive_lcs(Y, X, m-1, n-1); \n",
    "    else: \n",
    "        return max(recursive_lcs(Y, X, m, n-1), recursive_lcs(Y, X, m-1, n)); \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "list_a = ['F','DF','AGD', 'VYBI', 'DCYPS', 'KLMADT', 'OPYTESK', 'QTYSFGHS', 'HVBFLFKGN', 'AVBFLFKGNY', 'BVBFLFKGNQU', 'HVBFLFKGNPST']\n",
    "times = []\n",
    "length = []\n",
    "\n",
    "for i in range(len(list_a)):\n",
    "    \n",
    "    Alphabet_S = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    Input_S = list_a[i]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    recursive_lcs(Alphabet_S, Input_S, len(Alphabet_S), len(Input_S))\n",
    "    \n",
    "    t = (time.time() - start_time)\n",
    "    \n",
    "    l = len(Input_S)\n",
    "    times.append(t)\n",
    "    length.append(l)\n",
    "    \n",
    "plt.plot(length,times, linewidth = 3.0)\n",
    "plt.xlabel('Length of Query')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Execution Time & Length of Query Plot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideration:\n",
    "\n",
    "As we can see above the graph prove that the running time is increasing dramatically which really look like the recursive algorithm has the exponential time complexity.\n",
    "\n",
    "3. Write a program that computes the length of the subsequence of maximum length, using dynamic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n",
      "Please enter a sequence with upper cases: ABCYDEUFG\n",
      "Length of LCS is  7\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "def longest_common_subsequence(S1 , S2):\n",
    "    \n",
    "    m = len(S1) \n",
    "    n = len(S2) \n",
    "\n",
    "    # declaring the array for storing the dp values \n",
    "    L = [[None]*(n+1) for i in range(m+1)] \n",
    "\n",
    "    \"\"\"Following steps build L[m+1][n+1] in bottom up fashion \n",
    "    Note: L[i][j] contains length of LCS of X[0..i-1] \n",
    "    and Y[0..j-1]\"\"\"\n",
    "    for i in range(m+1): \n",
    "        for j in range(n+1): \n",
    "            if i == 0 or j == 0 : \n",
    "                L[i][j] = 0\n",
    "            elif S1[i-1] == S2[j-1]: \n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else: \n",
    "                L[i][j] = max(L[i-1][j] , L[i][j-1]) \n",
    "\n",
    "    # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1] \n",
    "    return L[m][n]\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "Input_S = input(\"Please enter a sequence with upper cases: \")\n",
    "Alphabet_S = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "if Input_S.isalpha() and Input_S.isupper():\n",
    "    print (\"Length of LCS is \", longest_common_subsequence(Input_S, Alphabet_S))\n",
    "else:\n",
    "    print ('Please enter a string with only upper case letters in English!')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations:\n",
    "As we have tried several times with short strings that can be proven whether the function is finding correct answer or not, our function has returned correct answer for all different inputs. Since we have conditioned the input that all strings can only contain the letters of English alphabet it also makes sure that we have a correct input. Howewer, the dynamic algorithm finds the longest subsequence really quick even for the string that has a really long length.\n",
    "\n",
    "4. What is its runtime complexity?\n",
    "\n",
    "### Consideration:\n",
    "We have tried the dynamic algorithm with different strings that have really long length but the algorithm has returned the result quickly since it has the O(mn) runtime complexity.\n",
    "\n",
    "Credit to: Back To Back SWE https://www.youtube.com/watch?v=ASoaQq66foQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
